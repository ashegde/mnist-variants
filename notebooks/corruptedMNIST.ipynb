{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we explore the \"Rotated MNIST + background images\" from https://sites.google.com/a/lisa.iro.umontreal.ca/public_static_twiki/variations-on-the-mnist-digits and build a Data Loader to process this into Dataset objects. Note that in an accompanying notebook, we use a similar neural architecture on the classical MNIST dataset and find very good performance. In fact, the training achieves $>95%$ accuracy on a validation batch within the first few hundred iterations (after around just 1 epoch). It appears challenging to achieve similar results on the version of the MNIST dataset used here.\n",
        "\n",
        "A wonderful reference for convolutional arithmetic is __Dumoulin, Vincent, and Francesco Visin. \"A guide to convolution arithmetic for deep learning.\" arXiv preprint arXiv:1603.07285 (2016)__ . One of the basic takeaways is that most of the dimensioning for multivariate tensors can be analyzed by considering single dimensions. This makes accounting for kernel sizes, strides, padding, pooling, etc. more intuitive.  \n"
      ],
      "metadata": {
        "id": "F7OlfKc4eaal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import argparse\n",
        "from typing import List, Dict, Any"
      ],
      "metadata": {
        "id": "g05caBL_pMqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseDataset(torch.utils.data.Dataset):\n",
        "\n",
        "  def __init__(self, x,y):\n",
        "    super().__init__()\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.x.shape[0]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    'produces a single data point'\n",
        "    xs = self.x[index]\n",
        "    ys = self.y[index]\n",
        "\n",
        "    return xs, ys\n",
        "\n",
        "class MNISTDataModule:\n",
        "  '''\n",
        "  Helper module that downloads, loads, and partitions the MNIST dataset.\n",
        "  '''\n",
        "  url = \"http://www.iro.umontreal.ca/~lisa/icml2007data/\"\n",
        "  filename = \"mnist_rotation_back_image_new.zip\"\n",
        "\n",
        "  def __init__(self, dir, batch_size=32):\n",
        "      self.dir = dir \n",
        "      self.batch_size = batch_size\n",
        "      self.path = self.dir + '/' +self.filename\n",
        "\n",
        "  def download_data(self):\n",
        "    # create directories and download dataset\n",
        "    if not os.path.exists(self.dir):\n",
        "      os.mkdir(self.dir)\n",
        "    if not os.path.exists(self.path):\n",
        "      content = requests.get(self.url + self.filename).content\n",
        "      with open(self.path, \"wb\") as f:\n",
        "        f.write(content)\n",
        "    with zipfile.ZipFile(self.path) as f:\n",
        "      f.extractall(path=self.dir)\n",
        "      \n",
        "  def setup(self):\n",
        "    # load data\n",
        "    with open(self.dir+'/mnist_all_background_images_rotation_normalized_test.amat', 'r') as f1:\n",
        "      ds_te = [[float(a) for a in line.split()] for line in f1]\n",
        "    with open(self.dir+'/mnist_all_background_images_rotation_normalized_train_valid.amat', 'r') as f2:\n",
        "      ds_tr_val = [[float(a) for a in line.split()]  for line in f2]\n",
        "      \n",
        "    ds_te, ds_tr_val = map(torch.tensor, (ds_te, ds_tr_val))\n",
        "\n",
        "    # hardwired 80%-20% split into training and validation\n",
        "    n1 = int(0.8*ds_tr_val.shape[0])\n",
        "\n",
        "    Xtr, Ytr = ds_tr_val[:n1,:-1], ds_tr_val[:n1,-1]\n",
        "    Xval, Yval = ds_tr_val[n1:,:-1], ds_tr_val[n1:,-1]\n",
        "    Xte, Yte = ds_te[:,:-1], ds_te[:,-1]\n",
        "    \n",
        "    self.train_ds = BaseDataset(Xtr, Ytr)\n",
        "    self.valid_ds = BaseDataset(Xval, Yval)\n",
        "    self.test_ds = BaseDataset(Xte, Yte)\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    return torch.utils.data.DataLoader(self.train_ds, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "  def val_dataloader(self):\n",
        "    return torch.utils.data.DataLoader(self.valid_ds, batch_size=3*self.batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "yQnh8UWR-Ey8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "dir = \"/content/data\"\n",
        "bs = 32\n",
        "dm = MNISTDataModule(dir, batch_size = bs)\n",
        "dm.download_data()"
      ],
      "metadata": {
        "id": "YVwuBXC6pUvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dm.setup()"
      ],
      "metadata": {
        "id": "pMgWh8DHDXJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dl = dm.train_dataloader()"
      ],
      "metadata": {
        "id": "1cYJx0BQAZvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load a batch of data\n",
        "dataiter = iter(train_dl)\n",
        "X,y = dataiter.next()"
      ],
      "metadata": {
        "id": "gaG1WTtHIx36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#verify that the data is normalized\n",
        "print(X.min().item(), X.max().item())"
      ],
      "metadata": {
        "id": "lo1AhnKqIz3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## let's look at some data samples\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15, 5), nrows=2, ncols=5)\n",
        "i = 0\n",
        "for row in ax:\n",
        "    for col in row:\n",
        "        \n",
        "        col.imshow(X[i].reshape(28,28), cmap='Greys')\n",
        "        col.set_title('label = '+str(y[i].item()))\n",
        "        col.set_axis_off()\n",
        "        i+=1\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fkmq_vTWAukw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The digits appear rotated and flipped/mirrored (both horizontally and vertically). Moreover, the digits have been overlayed on various textured backgrounds. Naturally, these backgrounds are irrelevant to the label. The combination of these corruptions renders some of the digits beyond human recognition. But are they beyond machine recognition?\n",
        "\n",
        "How should we think of this data? Let us suppose the data are random variables $X$ (image) and $Y$ (label) with unknown joint distribution $P_{XY}(x,y)$. We further assume that our training, validation, and test datasets are constructed from i.i.d. samples from $P$. The marginals $P_{X}(x)$, $P_{Y}(y)$ and conditional distributions $P_{X|Y}(x|Y=y)$, $P_{Y|X}(y|X=x)$ describe important information about the dataset. For example,\n",
        "$P(X|Y=y)$ describes the within-class variability in $X$. Presumably, this will be quite high given the differing backgrounds, rotations, and other transformations applied to the associated digit. Alternatively, $P(Y|X=x)$ is basically what we are trying to model in constructing a classifier. "
      ],
      "metadata": {
        "id": "rer_KHdonnTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#how balanced is this dataset? Approximate P_Y(y).\n",
        "Ytr = dm.train_ds.y\n",
        "label_freqs = torch.zeros(10, dtype=torch.float32)\n",
        "for y in Ytr:\n",
        "    label_freqs[int(y.item())] += 1.0\n",
        "label_freqs *= 1/Ytr.shape[0]\n",
        "plt.bar(range(10), label_freqs)\n",
        "plt.xticks(range(10))\n",
        "plt.title('histogram of labels')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uKFZE7RivfHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The implication of a balanced dataset is that a probability model in which $X$ and $Y$ are statistically independent and based purely on estimated marginals, i.e., $\\hat{P}_{XY}(x,y) = \\hat{P}_{X}(x) \\hat{P}_{Y}(y)$, will perform poorly on the training data. This is because any classifier based on the conditional $\\hat{P}_{Y|X}(y|X=x) = \\hat{P}_Y(y)$ is no better than guessing a label uniformly at random. Since such a trivial model performs poorly on the training dataset, we can expect our fitting process to avoid this configuration and produce a model that actually learns some of the dependence structure in the training data. As an aside, if we learn too much from the training dataset, then we are likely to overfit. \n",
        "\n",
        "Conversely, if the dataset was severly unbalanced, then the above trivial model could perform well on the training dataset by just guessing the labels that appear more frequently in the training dataset. Such a model is very easy to learn, but will not generalize beyond the training regime.\n",
        "\n",
        "Note that the model $\\hat{P}_{Y|X}(y|X=x) = \\hat{P}_Y(y)$ is perhaps the simplest model that one can build from the training data. In the context of MNIST, it contains only $10$ parameters -- the empirical probabilities of each of the $10$ class labels. "
      ],
      "metadata": {
        "id": "TCclIFFp92ce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the Neural Network"
      ],
      "metadata": {
        "id": "Y377_37cKvSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CONV_DIM = 64\n",
        "FC_DIM = 256\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Residual block with two 3x3 convolutional layers\n",
        "    Padding size 1 to preserve input dimensionality\n",
        "    ReLU nonlinearity\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, indim: int, outdim: int, ksize=3, s=1) -> None:\n",
        "        super().__init__()\n",
        "        #{in,out}dim = number of filters (each of size ksize_in) in the layer\n",
        "\n",
        "\n",
        "        self.conv1 = nn.Conv2d(indim, outdim, kernel_size=ksize, stride=s, padding=1)\n",
        "        self.bn = nn.BatchNorm2d(outdim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(outdim, outdim, kernel_size=ksize, stride=s, padding=1)\n",
        "        \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\".\n",
        "\n",
        "        Inputs\n",
        "        ----------\n",
        "        x\n",
        "            (B, C_in, H_in, W_in) tensor\n",
        "\n",
        "        Outputs\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            (B, C_out, H_out, W_out) tensor\n",
        "        \"\"\"\n",
        "        z = self.conv1(x)  # (B, C_out, H_in, W_in)\n",
        "        z = self.bn(z)     # (B, C_out, H_in, W_in)  \n",
        "        z = self.relu(z)   # (B, C_out, H_in, W_in)\n",
        "        z = self.conv2(z)  # (B, C_out , H_in, W_in) \n",
        "        outs = x+z\n",
        "        return outs\n",
        "\n"
      ],
      "metadata": {
        "id": "ZOaa4NfMKzQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple ResNet for the Rotated MNIST dataset\n",
        "    Recall, the MNIST dataset takes as input (channels, width, height) = (1,28,28) images, i.e., 784 dimensional feature vectors\n",
        "    and has 10 classes (for the digits 0,1,...,9)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, args: argparse.Namespace = None) -> None:\n",
        "        super().__init__()\n",
        "        self.args = vars(args) if args is not None else {}\n",
        "\n",
        "        #hardwired for MNIST\n",
        "        self.input_height, self.input_width = 28,28 \n",
        "        num_classes = 10\n",
        "\n",
        "        conv_dim = self.args.get(\"conv_dim\", CONV_DIM) #number of filters / channels\n",
        "        fc_dim = self.args.get(\"fc_dim\", FC_DIM)\n",
        "\n",
        "\n",
        "        ## input = (B,C=1,H,W), C=1 as MNIST has just a single channel\n",
        "        self.res1 = ResBlock(1, conv_dim)   \n",
        "        self.res2 = ResBlock(conv_dim, conv_dim)\n",
        "        self.max_pool = nn.MaxPool2d(2)\n",
        "        self.res3 = ResBlock(conv_dim, conv_dim)\n",
        "        self.res4 = ResBlock(conv_dim, conv_dim)\n",
        "        self.res5 = ResBlock(conv_dim, conv_dim)\n",
        "        self.res6 = ResBlock(conv_dim, conv_dim)\n",
        "        self.bn2d = nn.BatchNorm2d(conv_dim)\n",
        "        self.drop = nn.Dropout(0.5)\n",
        "        conv_output_height, conv_output_width = self.input_height // 4, self.input_width // 4\n",
        "        self.flatten = nn.Flatten()\n",
        "        fc_input_dim = int(conv_output_height * conv_output_width * conv_dim)\n",
        "        self.fc1 = nn.Linear(fc_input_dim, fc_dim)\n",
        "        self.bn1d = nn.BatchNorm1d(fc_dim)\n",
        "        self.fc2 = nn.Linear(fc_dim, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        #adjust inits\n",
        "        with torch.no_grad():\n",
        "          self.fc2.weight *= 0.001\n",
        "          self.fc2.bias *= 0.001\n",
        "        \n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        x\n",
        "            (B, Ch, H, W) tensor, where H and W must equal input height and width from data_config.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            (B, Classes) tensor\n",
        "        \"\"\"\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.res1(x)      # (B, CONV_DIM, H, W)\n",
        "        x = self.res2(x)      # (B, CONV_DIM, H, W)\n",
        "        x = self.max_pool(x)  # (B, CONV_DIM, H // 2, W // 2)\n",
        "        x = self.drop(x)\n",
        "        x = self.res3(x)      # (B, CONV_DIM, H // 2, W // 2)\n",
        "        x = self.res4(x)      # (B, CONV_DIM, H // 2, W // 2)\n",
        "        x = self.max_pool(x)  # (B, CONV_DIM, H // 4, W // 4)\n",
        "        x = self.drop(x)\n",
        "        x = self.res5(x)      # (B, CONV_DIM, H // 4, W // 4)\n",
        "        x = self.res6(x)      # (B, CONV_DIM, H // 4, W // 4)\n",
        "        x = self.bn2d(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.flatten(x)   # (B, CONV_DIM * H // 4 * W // 4)\n",
        "        x = self.fc1(x)       # (B, FC_DIM)\n",
        "        x = self.bn1d(x)      # (B, FC_DIM)\n",
        "        x = self.relu(x)      # (B, FC_DIM)\n",
        "        x = self.fc2(x)       # (B, Classes)\n",
        "        return x"
      ],
      "metadata": {
        "id": "2W1NpVp2K3v7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(out: torch.Tensor, yb: torch.Tensor) -> torch.Tensor:\n",
        "    preds = torch.argmax(out, dim=1) \n",
        "    return (preds == yb).float().mean() #converts from 0-1 bool to float and then averages = the fraction of correct classifications. "
      ],
      "metadata": {
        "id": "k78Y2eTdLHKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(net: nn.Module, config_opt: Dict[str, Any]) -> List:\n",
        "  train_dataloader = config_opt['train_dataloader']\n",
        "  val_dataloader = config_opt['val_dataloader']\n",
        "  epochs = config_opt['epochs']\n",
        "  lr = config_opt['lr']\n",
        "  wd = config_opt['wd']\n",
        "  loss_func = F.cross_entropy\n",
        "  optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=wd)\n",
        "\n",
        "  valiter = iter(val_dataloader)\n",
        "\n",
        "  net.train()\n",
        "  iters = 0\n",
        "  outfreq = 100\n",
        "  lossi=[]\n",
        "\n",
        "  for p in net.parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    for xb, yb in train_dataloader:\n",
        "      # xb = (B, 28*28), yb = (B,)\n",
        "      xb.unsqueeze(1) #unsqueezing in a channel dimension\n",
        "      xb = xb.view(-1,1,28,28)\n",
        "      yb = yb.long()\n",
        "      \n",
        "      logits = net(xb)\n",
        "      loss = loss_func(logits, yb)\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # tracking\n",
        "      lossi.append(loss.log10().item())\n",
        "\n",
        "      # outputs\n",
        "      if iters % outfreq == 0:\n",
        "        with torch.no_grad():\n",
        "\n",
        "          try:\n",
        "            xv,yv = next(valiter)\n",
        "          except StopIteration:\n",
        "            valiter = iter(val_dataloader)\n",
        "            xv,yv = next(valiter)\n",
        "          \n",
        "          xv.unsqueeze(1) #unsqueezing in a channel dimension\n",
        "          xv = xv.view(-1,1,28,28)\n",
        "          yv = yv.long()\n",
        "\n",
        "          net.eval()\n",
        "          logitsv = net(xv)\n",
        "          net.train()\n",
        "          lossv = loss_func(logitsv,yv)\n",
        "          accv = accuracy(logitsv, yv)\n",
        "          acctr = accuracy(logits, yb)\n",
        "          print(f'{iters:7d} | epoch {epoch:7d} | loss  {loss.item():.4f} (val: {lossv.item():.4f}) | acc {acctr:.4f} (val: {accv:.4})') \n",
        "      \n",
        "      iters +=1\n",
        "\n",
        "  return lossi"
      ],
      "metadata": {
        "id": "BJZHymDwL7WJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ResNet()\n",
        "print(sum(p.nelement() for p in model.parameters()))"
      ],
      "metadata": {
        "id": "n9n4nk1_zC0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dl = dm.train_dataloader()\n",
        "val_dl = dm.val_dataloader()\n",
        "#opts = {'epochs': 10, 'train_dataloader': train_dl, 'val_dataloader': val_dl, 'learning_rate': 3e-4}\n",
        "opts = {'epochs': 5, 'train_dataloader': train_dl, 'val_dataloader': val_dl, 'lr': 1e-4, 'wd': 1e-4}\n",
        "l = fit(model, opts)"
      ],
      "metadata": {
        "id": "y4i8tgu0lmiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#so far, the best model seems to have a validation accuracy of around 0.9 (on a chosen batch)"
      ],
      "metadata": {
        "id": "34KIMuFIMybW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def performance(net: nn.Module, datamodule: MNISTDataModule):\n",
        "  metrics = {'train': {}, 'val': {}, 'test': {}}\n",
        "\n",
        "  #colab crashes due to limited RAM on the full test set, so we will reduce the test set to the training size\n",
        "  # ds = {'train': (datamodule.train_ds.x, datamodule.train_ds.y),\n",
        "  #       'val': (datamodule.valid_ds.x, datamodule.valid_ds.y),\n",
        "  #       'test': (datamodule.test_ds.x, datamodule.test_ds.y)}\n",
        "  ds = {'train': (datamodule.train_ds.x, datamodule.train_ds.y),\n",
        "      'val': (datamodule.valid_ds.x, datamodule.valid_ds.y)}\n",
        "  train_length = ds['train'][0].shape[0] \n",
        "  ds['test'] = (datamodule.test_ds.x[:train_length], datamodule.test_ds.y[:train_length])\n",
        "\n",
        "  net.eval()\n",
        "  for key,data in ds.items():\n",
        "    x = data[0].unsqueeze(1)\n",
        "    y = data[1].long()\n",
        "    x = x.view(-1,1,28,28)\n",
        "    logits = net(x)\n",
        "    loss = F.cross_entropy(logits, y)\n",
        "    acc = accuracy(logits, y) \n",
        "    metrics[key] = {'loss': loss.item(), 'acc': acc}\n",
        "  return metrics\n",
        "\n",
        "\n",
        "perf = performance(model, dm)"
      ],
      "metadata": {
        "id": "Ius5Z7Tq33kY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## let's look at some example predictions from the test dataset\n",
        "Xt, yt = dm.test_ds.x, dm.test_ds.y\n",
        "\n",
        "model.eval()\n",
        "\n",
        "Xt = Xt[:32].unsqueeze(1)\n",
        "yt = yt[:32].long()\n",
        "Xt = Xt.view(-1,1,28,28)\n",
        "logits = model(Xt) # (B, num_classes)\n",
        "ypred = torch.argmax(logits, dim=1) #(B,) \n",
        "\n",
        "\n",
        "Xt = Xt.squeeze(1)\n",
        "fig, ax = plt.subplots(figsize=(15, 5), nrows=2, ncols=5)\n",
        "i = 0\n",
        "for row in ax:\n",
        "    for col in row:\n",
        "        col.imshow(Xt[i].reshape(28,28), cmap='Greys')\n",
        "        col.set_title('pred = '+str(ypred[i].item()))\n",
        "        col.set_axis_off()\n",
        "        i+=1\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fQLdUpqlptgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While the results are good, a natural question to ask is what exactly did the model learn? Specifically, what do the filters in each convolutional layer look like? For a given layer, are they diverse or redundant?"
      ],
      "metadata": {
        "id": "LFZharpklncr"
      }
    }
  ]
}